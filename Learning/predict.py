import utils
from SupervisedLearning import SKSupervisedLearning
from train_files import TrainFiles
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import SVC
from os import path
from sklearn.metrics import log_loss
from sklearn.decomposition.pca import PCA

#tf = TrainFiles("/kaggle/malware/train/1dlbp", "/kaggle/malware/test/1dlbp", "/kaggle/malware/trainLabels.csv", validate = False)
#tf = TrainFiles('/kaggle/malware/scratchpad/train/1dlbp', '/kaggle/malware/scratchpad/test/1dlbp', "/kaggle/malware/trainLabels.csv", validate = True)
tf = TrainFiles('/kaggle/malware/scratchpad/text/train/instr_freq', '/kaggle/malware/scratchpad/text/test/instr_freq', "/kaggle/malware/trainLabels.csv", validate = True)

X_train, Y_train, X_test, Y_test = tf.prepare_inputs_csv()

doSelection = False
prediction = False
def predict(X_train, Y_train, X_test):
    # no validation labels on actual prediction
    
    # random forest
    sl_rfc = SKSupervisedLearning(RandomForestClassifier, X_train, Y_train, X_test, Y_test)
    sl_rfc.fit_standard_scaler()
    sl_rfc.train_params = {'max_depth': 100, 'n_estimators': 7500}

    print "Starting on RF: ", utils.time_now_str()

    _, ll_rfc = sl_rfc.fit_and_validate()
    print "RF score: {0:.4f}".format(ll_rfc)

    # trees

    sl_trees = SKSupervisedLearning(ExtraTreesClassifier, X_train, Y_train, X_test, Y_test)
    sl_trees.fit_standard_scaler()
    sl_trees.train_params =  {'n_estimators': 7500}
    
    print "Starting on trees: ", utils.time_now_str()

    _, ll_trees = sl_trees.fit_and_validate()
    print "Trees score: {0:.4f}".format(ll_trees)
    if doSelection:

        # selecte important trees features
        selected = np.where(sl_trees.clf.feature_importances_ > 0.003)[0]

        X_train = sl_trees.X_train_scaled[:, selected]
        X_test = sl_trees.X_test_scaled[:, selected]

    sl_svm = SKSupervisedLearning(SVC, X_train, Y_train, X_test, Y_test)
    sl_svm.fit_standard_scaler()
    sl_svm.train_params = {'C': 100, 'gamma': 0.1, 'probability': True}

    print "Starting SVM: ", utils.time_now_str()
    _, ll_svm = sl_svm.fit_and_validate()
    print "SVM score: {0:.4f}".format(ll_svm)
    print "Finished training: ", utils.time_now_str()

    if prediction:
        proba = utils.vote_reduce([sl_svm.proba_test, sl_rfc.proba_test], [1./2., 1./2.])

        out_labels = "/kaggle/malware/submission18.csv"
        task_labels = "/kaggle/malware/testLabels.csv"
        labels = [path.splitext(t)[0] for t in tf.get_val_inputs()]
        utils.write_to_csv(task_labels, labels, proba, out_labels)

    else:
        # visualize the decision surface, projected down to the first
        # two principal components of the dataset
        pca = PCA(n_components=2).fit(sl_svm.X_train_scaled)

        X = pca.transform(sl_svm.X_train_scaled)

        x = np.arange(X[:, 0].min() - 1, X[:, 1].max() + 1, 1)
        y = np.arange(X[:, 1].min() - 1, X[:, 1].max() + 1, 1)

        xx, yy = np.meshgrid(x, y)

        # title for the plots
        titles = ['SVC with rbf kernel',
                  'Random Forest \n'
                  'n_components=7500',
                  'Decision Tres \n'
                  'n_components=7500']

        #plt.tight_layout()
        plt.figure(figsize=(12, 5))

        # predict and plot
        for i, clf in enumerate((sl_svm.clf, sl_rfc.clf, sl_trees.clf)):
            # Plot the decision boundary. For that, we will assign a color to each
            # point in the mesh [x_min, m_max]x[y_min, y_max].
            plt.subplot(1, 3, i + 1)
            clf.fit(X, Y_train)
            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

            # Put the result into a color plot
            Z = Z.reshape(xx.shape)
            plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)
            plt.axis('off')

            # Plot also the training points
            plt.scatter(X[:, 0], X[:, 1], c=Y_train, cmap=plt.cm.Paired)

            plt.title(titles[i])
        plt.tight_layout()
        plt.show()
