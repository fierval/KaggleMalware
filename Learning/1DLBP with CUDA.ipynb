{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a 1D Local Binary Pattern Histogram on NVIDIA GPU with CUDA and Numbapro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was done for the Microsoft Malware competition on Kaggle. \n",
    "In this contest, a bunch of malware files needed to be classified in 9 categories. The files were presented in two sets, 10,868 each: \n",
    "- text (disassembly of the malware)\n",
    "- binary (actual binaries minus PE header to render them harmless)\n",
    "\n",
    "I chose the [1DLBP](http://www.thinkmind.org/download.php?articleid=future_computing_2013_1_30_30012) algorithm described in the linked paper to extract (hopefully meaningful) features from each binary file. These features, in combination with some text-based features extracted from the disassemblies, proved to be quite effective in training an accurate classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- basic knowledge of CUDA programming (kernels, grid, blocks, threads, etc)\n",
    "- numba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numba import *\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm in Brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the paper (above) does a great job explaining the algorithm, I will only describe it very briefly.\n",
    "\n",
    "For every byte (b) in the image, we look at 4 (or any neighborhood) of bytes immediately to the \"left\" and at 4 bytes immediately to the \"right\" of the current byte.\n",
    "\n",
    "We now have 8 selected bytes, call them c[], where size(c) = 8. Each of the elements of array c is converted into a binary digit, based on the following rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center:  220\n",
      "Bytes around it:  [ 32   7 215 187  12  67 234 215]\n",
      "Pattern:  [0 0 0 0 0 0 1 0]\n",
      "As number base 10:  64\n"
     ]
    }
   ],
   "source": [
    "c = np.random.randint(0, 256, size=8) #neighborhood bytes to the left and to the right contain these values\n",
    "b = np.random.randint(256) # center of the neighborhood\n",
    "digits = np.array([0 if d < b else 1 for d in c]) # this is the number that represents the pattern\n",
    "print \"Center: \", b\n",
    "print \"Bytes around it: \", c\n",
    "print \"Pattern: \", digits \n",
    "\n",
    "# In order to compute the actual pattern, we just mulitply each of the digits by a power of two and add them up\n",
    "powers = 1 << np.arange(0, 8)\n",
    "print \"As number base 10: \", np.dot(powers, digits.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is actually a non-negative integer less than $2^8$.\n",
    "\n",
    "Then a histogram of these values is built. It has the size of 256 ints and in training experiments proves to be much more effective than a simple byte histogram.\n",
    "\n",
    "The functions below translate this description into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_1dlbp_cpu(input, neighborhood, p):\n",
    "    \"\"\"\n",
    "    Extract the 1d lbp pattern on CPU\n",
    "    \"\"\"\n",
    "    res = np.zeros(1 << (2 * neighborhood))\n",
    "    for i in range(neighborhood, len(input) - neighborhood):\n",
    "        left = input[i - neighborhood : i]\n",
    "        right = input[i + 1 : i + neighborhood + 1]\n",
    "        both = np.r_[left, right]\n",
    "        res[np.sum(p [both >= input[i]])] += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With numba, this can be expressed on the GPU using CUDA.\n",
    "If you have numbapro, CUDA support can be validated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------libraries detection-------------------------------\n",
      "Finding cublas\n",
      "\tlocated at /home/boris/anaconda/lib/libcublas.so.6.0.37\n",
      "\ttrying to open library...\tok\n",
      "Finding cusparse\n",
      "\tlocated at /home/boris/anaconda/lib/libcusparse.so.6.0.37\n",
      "\ttrying to open library...\tok\n",
      "Finding cufft\n",
      "\tlocated at /home/boris/anaconda/lib/libcufft.so.6.0.37\n",
      "\ttrying to open library...\tok\n",
      "Finding curand\n",
      "\tlocated at /home/boris/anaconda/lib/libcurand.so.6.0.37\n",
      "\ttrying to open library...\tok\n",
      "Finding nvvm\n",
      "\tlocated at /home/boris/anaconda/lib/libnvvm.so.2.0.0\n",
      "\ttrying to open library...\tok\n",
      "\tfinding libdevice for compute_20...\tok\n",
      "\tfinding libdevice for compute_30...\tok\n",
      "\tfinding libdevice for compute_35...\tok\n",
      "-------------------------------hardware detection-------------------------------\n",
      "Found 1 CUDA devices\n",
      "id 0        GeForce GTX 850M                              [SUPPORTED]\n",
      "                      compute capability: 5.0\n",
      "                           pci device id: 0\n",
      "                              pci bus id: 1\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "PASSED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numbapro\n",
    "numbapro.check_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am working with GTX Titan, 14 SMs (14 * 192 = 2688 SPs), 6 Gb RAM. This used to be cool a year and a half ago, but a lot has happened since then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###The Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cuda.jit('void(uint8[:], int32, int32[:], int32[:])')\n",
    "def lbp_kernel(input, neighborhood, powers, h):\n",
    "    i = cuda.grid(1)\n",
    "    r = 0\n",
    "    if i < input.shape[0] - 2 * neighborhood:\n",
    "        i += neighborhood\n",
    "        for j in range(i - neighborhood, i):\n",
    "            if input[j] >= input[i]:\n",
    "                r += powers[j - i + neighborhood]\n",
    "    \n",
    "        for j in range(i + 1, i + neighborhood + 1):\n",
    "            if input[j] >= input[i]:\n",
    "                r += powers[j - i + neighborhood - 1]\n",
    "\n",
    "        cuda.atomic.add(h, r, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a regular CUDA kernel. It almost looksy \"C++y\". Each thread is tasked with computing the pattern (\"digits\" above) around one single element of the original input array. We compute the pattern, then wait for all of the threads in the block to finish and update the histogram with their results.\n",
    "\n",
    "__Note:__ implementation of histogramming using atomics is the most straightforward, but not the most efficient one. For our purposes it will do just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Calling the Kernel\n",
    "That is pretty standard: allocate memory and move it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_1dlbp_gpu(input, neighborhood, d_powers):\n",
    "    '''\n",
    "    input - the input array\n",
    "    neighborhood - size of the neighborhood \n",
    "    d_powers - device address of the powers of two constants used to compute the final pattern\n",
    "    '''\n",
    "    maxThread = 512\n",
    "    blockDim = maxThread\n",
    "    d_input = cuda.to_device(input)\n",
    "\n",
    "    hist = np.zeros(2 ** (2 * neighborhood), dtype='int32')\n",
    "    gridDim = (len(input) - 2 * neighborhood + blockDim) / blockDim\n",
    "\n",
    "    d_hist = cuda.to_device(hist)\n",
    "\n",
    "    lbp_kernel[gridDim, blockDim](d_input, neighborhood, d_powers, d_hist)\n",
    "    d_hist.to_host()\n",
    "    return hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Testing\n",
    "\n",
    "Running comparison for arrays of several lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished on CPU: length: 1000, time: 0.0172s\n",
      "Finished on GPU: length: 1000, time: 0.0008s\n",
      "All h_cpu == h_gpu:  True\n",
      "Finished on CPU: length: 10000, time: 0.1596s\n",
      "Finished on GPU: length: 10000, time: 0.0008s\n",
      "All h_cpu == h_gpu:  True\n",
      "Finished on CPU: length: 100000, time: 1.6021s\n",
      "Finished on GPU: length: 100000, time: 0.0010s\n",
      "All h_cpu == h_gpu:  True\n",
      "Finished on CPU: length: 1000000, time: 16.1255s\n",
      "Finished on GPU: length: 1000000, time: 0.0031s\n",
      "All h_cpu == h_gpu:  True\n"
     ]
    }
   ],
   "source": [
    "X = np.arange(3, 7)\n",
    "X = 10 ** X\n",
    "neighborhood = 4\n",
    "\n",
    "cpu_times = np.zeros(X.shape[0])\n",
    "gpu_times = np.zeros(X.shape[0])\n",
    "\n",
    "p = 1 << np.array(range(0, 2 * neighborhood), dtype='int32')\n",
    "d_powers = cuda.to_device(p)\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    input = np.random.randint(0, 256, size = x).astype(np.uint8)\n",
    "    start = timer()\n",
    "    h_cpu = extract_1dlbp_cpu(input, neighborhood, p)\n",
    "    cpu_times[i] = timer() - start\n",
    "    print \"Finished on CPU: length: {0}, time: {1:3.4f}s\".format(x, cpu_times[i])\n",
    "\n",
    "    start = timer()\n",
    "    h_gpu = extract_1dlbp_gpu(input, neighborhood, d_powers)\n",
    "    gpu_times[i] = timer() - start\n",
    "    print \"Finished on GPU: length: {0}, time: {1:3.4f}s\".format(x, gpu_times[i])\n",
    "    print \"All h_cpu == h_gpu: \", (h_cpu == h_gpu).all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Charting\n",
    "Here both axes are log-scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(X, cpu_times, label = \"CPU\")\n",
    "plt.plot(X, gpu_times, label = \"GPU\")\n",
    "plt.xlabel('input length')\n",
    "plt.ylabel('time, sec')\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Improvements\n",
    "The above results _are_ a bit extreme and very boring. GPU is faster than CPU, however, 4 orders of magnitue?! Can we do better on CPU? Obviously the slowdown is due to Python complexity: array manipulations we are using for once... \n",
    "Here are things to try:\n",
    "- Mimic CUDA kernel on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_1dlbp_gpu_debug(input, neighborhood, powers, res):\n",
    "    maxThread = 512\n",
    "    blockDim = maxThread\n",
    "    gridDim = (len(input) - 2 * neighborhood + blockDim) / blockDim\n",
    "    \n",
    "    for block in range(0, gridDim):\n",
    "        for thread in range(0, blockDim):\n",
    "            r = 0\n",
    "            i = blockDim * block + thread\n",
    "            if i < input.shape[0] - 2 * neighborhood:\n",
    "                i += neighborhood\n",
    "                for j in range(i - neighborhood, i):\n",
    "                    if input[j] >= input[i]:\n",
    "                        r += powers[j - i + neighborhood]\n",
    "    \n",
    "                for j in range(i + 1, i + neighborhood + 1):\n",
    "                    if input[j] >= input[i]:\n",
    "                        r += powers[j - i + neighborhood - 1]\n",
    "\n",
    "                res[r] += 1\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good thing, too, because this is actually a way to debug a CUDA kernel in Python\n",
    "\n",
    "- Use numba to compile this simple code to something non-Python. For this we just decorate the definition like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit(\"int32[:](uint8[:], int64, int32[:], int32[:])\", nopython=True)\n",
    "def extract_1dlbp_cpu_jit(input, neighborhood, powers, res):\n",
    "    maxThread = 512\n",
    "    blockDim = maxThread\n",
    "    gridDim = (len(input) - 2 * neighborhood + blockDim) / blockDim\n",
    "    \n",
    "    for block in range(0, gridDim):\n",
    "        for thread in range(0, blockDim):\n",
    "            r = 0\n",
    "            i = blockDim * block + thread\n",
    "            if i < input.shape[0] - 2 * neighborhood:\n",
    "                i += neighborhood\n",
    "                for j in range(i - neighborhood, i):\n",
    "                    if input[j] >= input[i]:\n",
    "                        r += powers[j - i + neighborhood]\n",
    "    \n",
    "                for j in range(i + 1, i + neighborhood + 1):\n",
    "                    if input[j] >= input[i]:\n",
    "                        r += powers[j - i + neighborhood - 1]\n",
    "\n",
    "                res[r] += 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we modify the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1000\n",
      "--------------\n",
      "Finished on CPU: time: 0.01700s\n",
      "Finished on CPU (simple): time: 0.00790s\n",
      "Finished on CPU (numba: jit): time: 0.00049s\n",
      "Finished on GPU: time: 0.00060s\n",
      "All h_cpu == h_gpu:  True\n",
      "\n",
      "Length: 10000\n",
      "--------------\n",
      "Finished on CPU: time: 0.16169s\n",
      "Finished on CPU (simple): time: 0.08367s\n",
      "Finished on CPU (numba: jit): time: 0.00054s\n",
      "Finished on GPU: time: 0.00096s\n",
      "All h_cpu == h_gpu:  True\n",
      "\n",
      "Length: 100000\n",
      "--------------\n",
      "Finished on CPU: time: 1.62270s\n",
      "Finished on CPU (simple): time: 0.78276s\n",
      "Finished on CPU (numba: jit): time: 0.00428s\n",
      "Finished on GPU: time: 0.00114s\n",
      "All h_cpu == h_gpu:  True\n",
      "\n",
      "Length: 1000000\n",
      "--------------\n",
      "Finished on CPU: time: 15.99900s\n",
      "Finished on CPU (simple): time: 7.42127s\n",
      "Finished on CPU (numba: jit): time: 0.04031s\n",
      "Finished on GPU: time: 0.00287s\n",
      "All h_cpu == h_gpu:  True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "X = np.arange(3, 7)\n",
    "X = 10 ** X\n",
    "neighborhood = 4\n",
    "\n",
    "cpu_times = np.zeros(X.shape[0])\n",
    "cpu_times_simple = cpu_times.copy()\n",
    "cpu_times_jit = cpu_times.copy()\n",
    "gpu_times = np.zeros(X.shape[0])\n",
    "\n",
    "p = 1 << np.array(range(0, 2 * neighborhood), dtype='int32')\n",
    "d_powers = cuda.to_device(p)\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    input = np.random.randint(0, 256, size = x).astype(np.uint8)\n",
    "\n",
    "    print \"Length: {0}\".format(x)\n",
    "    print \"--------------\"\n",
    "\n",
    "    start = timer()\n",
    "    h_cpu = extract_1dlbp_cpu(input, neighborhood, p)\n",
    "    cpu_times[i] = timer() - start\n",
    "    print \"Finished on CPU: time: {0:3.5f}s\".format(cpu_times[i])\n",
    "\n",
    "    res = np.zeros(1 << (2 * neighborhood), dtype='int32')\n",
    "    start = timer()\n",
    "    h_cpu_simple = extract_1dlbp_gpu_debug(input, neighborhood, p, res)\n",
    "    cpu_times_simple[i] = timer() - start\n",
    "    print \"Finished on CPU (simple): time: {0:3.5f}s\".format(cpu_times_simple[i])\n",
    "\n",
    "    res = np.zeros(1 << (2 * neighborhood), dtype='int32')\n",
    "    start = timer()\n",
    "    h_cpu_jit = extract_1dlbp_cpu_jit(input, neighborhood, p, res)\n",
    "    cpu_times_jit[i] = timer() - start\n",
    "    print \"Finished on CPU (numba: jit): time: {0:3.5f}s\".format(cpu_times_jit[i])\n",
    "\n",
    "    start = timer()\n",
    "    h_gpu = extract_1dlbp_gpu(input, neighborhood, d_powers)\n",
    "    gpu_times[i] = timer() - start\n",
    "    print \"Finished on GPU: time: {0:3.5f}s\".format(gpu_times[i])\n",
    "    print \"All h_cpu == h_gpu: \", (h_cpu_jit == h_gpu).all() and (h_cpu_simple == h_cpu_jit).all() and (h_cpu == h_cpu_jit).all()\n",
    "    print ''\n",
    "\n",
    "f = plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(X, cpu_times, label = \"CPU\")\n",
    "plt.plot(X, cpu_times_simple, label = \"CPU non-vectorized\")\n",
    "plt.plot(X, cpu_times_jit, label = \"CPU jit\")\n",
    "plt.plot(X, gpu_times, label = \"GPU\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('input length')\n",
    "plt.ylabel('time, sec')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are cooking! This grapph is much more interesting than the one above.\n",
    "GPU is still the winner, but, as expected, the investment of moving things between RAM and GPU memory pays off once the size of our data is sufficiently large. The GPU doesn't get out of bed for anything smaller than about $10^5$. For anything else, if perf is crucial (as it very often is), we are better off writing it in a simple way and jitting. In other words: the best thing is not to write it in Python, but to do it in C++ from the get-go instead. Which appears to be a foregone conclusion, of course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
