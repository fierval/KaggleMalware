from pybrain.datasets            import ClassificationDataSet, UnsupervisedDataSet
from pybrain.utilities           import percentError
from pybrain.tools.shortcuts     import buildNetwork

from pybrain.supervised.trainers import BackpropTrainer
from pybrain.unsupervised.trainers.deepbelief import DeepBeliefTrainer
from pybrain.structure.networks import FeedForwardNetwork
from pybrain.structure.modules import SigmoidLayer, SoftmaxLayer

from tr_utils import append_to_arr
from train_files import TrainFiles

import numpy as np
import time
from SupervisedLearning import SKSupervisedLearning

from sklearn.metrics import log_loss
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

import matplotlib.pyplot as plt
from pybrain.structure.modules.linearlayer import LinearLayer

from scipy import stats
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.structure.connections.full import FullConnection
from pybrain.structure.modules.biasunit import BiasUnit
from pybrain.unsupervised.trainers.rbm import RbmGibbsTrainerConfig, RbmGibbsTrainer, RbmGaussTrainer, RbmBernoulliTrainer
from pybrain.structure.networks.rbm import Rbm
from RBM import RBM


train_path = "/kaggle/malware/scratchpad/train/mix_lbp_200"
test_path = "/kaggle/malware/scratchpad/test/mix_lbp_200"
labels_file = "/kaggle/malware/trainLabels.csv"

def _createDataSet(X, Y, one_based):
    labels = np.unique(Y)
    alldata = ClassificationDataSet(X.shape[1], nb_classes = labels.shape[0], class_labels = labels)
    shift = 1 if one_based else 0
    for i in range(X.shape[0]):
        alldata.addSample(X[i], Y[i] - shift)
    
    alldata._convertToOneOfMany()
    return alldata

def _createUnsupervisedDataSet(X):
    alldata = UnsupervisedDataSet(X.shape[1])
    for i in X:
        alldata.addSample(i)
    return alldata

def createDataSets(X_train, Y_train, X_test, Y_test, one_based = True):
    """
    Creates the data set. Handles one-based classifications (PyBrain uses zero-based ones).
    """
    trndata = _createDataSet(X_train, Y_train, one_based)
    tstdata = _createDataSet(X_test, Y_test, one_based)    
    return trndata, tstdata

def nn_log_loss(fnn, data):
    proba = fnn.activateOnDataset(data)
    return log_loss(data['target'], proba)

def train(trndata, tstdata, epochs = 100, test_error = 0.2, weight_decay = 0.0001, momentum = 0.5):
    """
    FF neural net
    """

    fnn = buildNetwork(trndata.indim, trndata.indim / 4, trndata.outdim, outclass = SoftmaxLayer)

    trainer = BackpropTrainer(fnn, trndata, momentum = momentum, weightdecay = weight_decay)

    epoch_delta = 1
    stop = False
    
    trnResults = np.array([])
    tstResults = np.array([])
    totEpochs = np.array([])

    trnLogLoss = np.array([])
    tstLogLoss = np.array([])

    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,8))
    #hold(True) # overplot on

    #plt.ion()
    while not stop:
        trainer.trainEpochs(epoch_delta)

        trnresult = percentError( trainer.testOnClassData(),
                              trndata['class'] )
        tstresult = percentError( trainer.testOnClassData(
           dataset=tstdata ), tstdata['class'] )

        tstLogLoss = append_to_arr(tstLogLoss, nn_log_loss(fnn, tstdata))
        trnLogLoss = append_to_arr(trnLogLoss, nn_log_loss(fnn, trndata))

        print "epoch: %4d" % trainer.totalepochs, \
          "  train error: %5.2f%%" % trnresult, \
          "  test error: %5.2f%%" % tstresult, \
          " test logloss: %2.4f" % tstLogLoss[-1], \
          " train logloss: %2.4f" % trnLogLoss[-1]

        
        trnResults = append_to_arr(trnResults, trnresult)
        tstResults = append_to_arr(tstResults, tstresult)
        totEpochs = append_to_arr(totEpochs, trainer.totalepochs)
        
        plt.sca(ax1)
        plt.cla()
        ax1.plot(totEpochs, trnResults, label = 'Train')
        ax1.plot(totEpochs, tstResults, label = 'Test')

        plt.sca(ax2)
        plt.cla()
        ax2.plot(totEpochs, trnLogLoss, label = 'Train')
        ax2.plot(totEpochs, tstLogLoss, label = 'Test')

        ax1.legend()
        ax2.legend()

        plt.draw()
        time.sleep(0.1)
        plt.pause(0.0001)

        stop = (tstLogLoss <= test_error or trainer.totalepochs >= epochs)
    return fnn

def do_train():
    tf = TrainFiles(train_path, test_path, labels_file)
    X, Y, Xt, Yt = tf.prepare_inputs()
    sl = SKSupervisedLearning(SVC, X, Y, Xt, Yt)
    sl.fit_standard_scaler()

    #construct a dataset for RBM
    X_rbm = X[:, 257:]
    Xt_rbm = X[:, 257:]

    rng = np.random.RandomState(123)
    rbm = RBM(X_rbm, n_visible=X_rbm.shape[1], n_hidden=X_rbm.shape[1]/4, numpy_rng=rng)

    pretrain_lr = 0.1
    k = 2
    pretraining_epochs = 200
    for epoch in xrange(pretraining_epochs):
        rbm.contrastive_divergence(lr=pretrain_lr, k=k)
        cost = rbm.get_reconstruction_cross_entropy()
        print >> sys.stderr, 'Training epoch %d, cost is ' % epoch, cost


    trndata, tstdata = createDataSets(sl.X_train_scaled, Y, sl.X_test_scaled, Yt)
    fnn = train(trndata, tstdata, epochs = 1000, test_error = 0.02, momentum = 0.5, weight_decay = 0.0001)
